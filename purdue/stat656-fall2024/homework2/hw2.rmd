---
title: |
    | **STAT 656: Bayesian Data Analysis**
    | **Fall 2024**
    | **Homework 2**
author: "Juanwu Lu^[College of Engineering, Purdue University, West Lafayette, IN, USA]"
description: "Homework 2 for STAT 656 at Purdue University"
output: pdf_document
---
```{r message = FALSE}
library("bayesplot")
library("ggplot2")
library("patchwork")
library("rstan")
options(repr.plot.width = 6, repr.plot.height = 4)
bayesplot_theme_set(theme_default(base_size = 12, base_family = "sans"))
```

# Synthetic Data

The file `hw2_synthetic.csv` is a dataset of count-valued measurements $\boldsymbol{y}=\left\{y_1,\ldots,y_n\right\}$, with $y_i\in{0, 1,\ldots}$. Each output $y_{i}$ has an associated $x_{i}=(x_{i,1},x_{i,2})\in\mathbb{R}^{2}$, and write $\boldsymbol{x} = \left\{x_{1},\ldots,x_{n}\right\}$'s as $\boldsymbol{x}$. We model $y_{i}$ as
$$
    y_{i}|\beta\sim\text{Poisson}(e^{f(x_{i},\beta)}).
$$
Here, the exponential is to ensure the Poisson rate is always positive, and the function $f(x_{i},\beta) = \beta_{0} + \beta_{1}x_{i,1} + \beta_{2}x_{i,2} + \beta_{3}x^{2}_{i,1} + \beta_{4}x^{2}_{i,2} + \beta_{5}x_{i,1}x_{i,2}$.

1. (25 points) With the provided data, perform a Bayesian analysis on the parameters of the model above to decide **which terms in the expression for $f(x)$ you think are important**. State clearly what your prior over $\beta$ is, and how you arrived at your conclusion, including any useful figures (especially of the posterior distribution). You can use Stan.

    **Solution**:

    ```{r}
    # Read the data
    if (file.exists("data/hw2_synthetic.csv")) {
      data <- read.csv("data/hw2_synthetic.csv", header = TRUE)
    } else {
      stop("FileNotFound: data file not found at 'data/hw2_synthetic.csv'.")
    }
    summary(data)
    ```

    Since the model is a Poisson regression, the logarithm of the response variable is assumed to be a linear combination of the kernel features. The visualization below shows the relationship between the logarithm of the response variable and the kernel features. It is shown that $\log{y}$ is roughly positively correlated with $x_{1}$ and $x_{1}x_{2}$, and roughly negatively correlated with $x_{2}$, $x_{1}^{2}$, and $x_{2}^{2}$.
    ```{r}
    # Preprocess data to create the kernel terms
    data$x1_sq <- data$x1^2
    data$x2_sq <- data$x2^2
    data$x1_x2 <- data$x1 * data$x2
    data$log_y <- log(data$y + 1)
    data <- data[, c("x1", "x2", "x1_sq", "x2_sq", "x1_x2", "y", "log_y")]
    plot(data[, c("x1", "x2", "x1_sq", "x2_sq", "x1_x2", "log_y")])
    ```

    Therefore, the weights $\beta_{i}$ can be both positive and negative. Given no prior knowledge on the features, I choose a non-informative prior for the weights, _i.e.,_ the isotropic normal distribution: $\mathcal{N}(0, 10^2\mathbf{I})$. The Stan model is implemented as follows:

    ```{r message = FALSE, results = "hide"}
    linreg_poisson_code <- "
        // Input arguments to the model
        data {
            int<lower=0> n;         // Number of observations
            int<lower=0> k;         // Number of features
            real<lower=0> pr_std;   // Prior coefficients standard deviation
            matrix[n, k] x;         // Observation matrix
            int<lower=0> y[n];                  // Integer response vector
        }

        // Latent parameters of interests
        parameters {
            vector[k] beta;         // Coefficients
        }

        // Transformed parameters for MCMC sampling
        transformed parameters {
            vector[n] lambda = exp(x * beta);    // Poisson rate
        }

        // PoissoXn Linear Regression Model
        model {
            beta ~ normal(0, pr_std);           // Prior on the coefficients
            y ~ poisson(lambda);                // Poisson emission
        }

        // Retrieve MCMC samples
        generated quantities {
            real y_hat[n];
            y_hat = poisson_rng(lambda);
        }
    "
    linreg_poisson_model <- stan_model(
      model_name = "poisson_regression",
      model_code = linreg_poisson_code
    )
    x <- data[, c("x1", "x2", "x1_sq", "x2_sq", "x1_x2")]
    x[, "intercept"] <- 1
    y <- data$y
    reg_data <- list(n = nrow(x), k = ncol(x), pr_std = 10.0, x = x, y = y)
    nfit <- sampling(
      linreg_poisson_model,
      data = reg_data,
      iter = 10000,
      warmup = 2000,
      chains = 1,
      seed = 42,
      show_messages = FALSE,
    )
    ```

    The visualization below shows the posterior distributions of the coefficients $\boldsymbol{\beta} = \left\{\beta_{0},\ldots,\beta_{5}\right\}$ with 95\% intervals. Based on the results, the coefficients of $x_{1}^2$, and $x_{1}x_{2}$ are close to zero (_i.e.,_ with zero within its 95\% posterior interval), while the coefficients of $x_{1}$, $x_{2}$, and $x_{2}^2$ are significantly different from zero. Therefore, the terms $x_{1}$, $x_{2}$, and $x_{2}^2$ are important in the expression for $f(x)$.

    ```{r fig.cap = "Posterior Distributions of the Coefficients.", message = FALSE}
    samples <- as.data.frame(nfit)
    colnames(samples)[seq_len(ncol(x))] <- colnames(x)
    mcmc_areas(
      samples[, seq_len(ncol(x))],
      pars = colnames(x),
      prob = 0.95,
    )
    ```

2. (25 points) Having decided which terms in $f$ are important, keep only those and discard the rest, resulting in a possibly simpler model. Now perform a Bayesian analysis over the parameters of this model. Note that you are using the data twice, once to select the model and next to fit it, but we will not worry about that. Compare the posteriors for both models.

    **Solution**:

    Based on the analysis in the previous question, the terms $x_{1}$, $x_{2}$, and $x_{2}^2$ are kept in the model. The new stan model is implemented as follows:

    ```{r message = FALSE, results = "hide"}
    x_simple <- x[, c("x1", "x2", "x2_sq", "intercept")]
    reg_data_simple <- list(
      n = nrow(x_simple),
      k = ncol(x_simple),
      pr_std = 10.0,
      x = x_simple,
      y = y
    )
    nfit_simple <- sampling(
      linreg_poisson_model,
      data = reg_data_simple,
      iter = 10000,
      warmup = 2000,
      chains = 1,
      seed = 42,
      show_messages = FALSE,
    )
    ```

    The visualization below shows the posterior distributions of the coefficients $\boldsymbol{\beta}^\prime = \left\{\beta_{0},\beta_{1},\beta_{2},\beta_{4}\right\}$ with 95\% intervals. The results show that the coefficients of $x_{1}$, $x_{2}$, and $x_{2}^2$ are all significantly different from zero (_i.e.,_ with zero outside its 95% posterior interval).

    ```{r fig.cap = "Posterior Distributions of the Coefficients.", message = FALSE}
    samples_simple <- as.data.frame(nfit_simple)
    colnames(samples_simple)[seq_len(ncol(x_simple))] <- colnames(x_simple)
    mcmc_areas(
      samples_simple[, seq_len(ncol(x_simple))],
      pars = colnames(x_simple),
      prob = 0.95,
    )
    ```

    Compared to the model from the previous question, the posterior distributions of the coefficients in this simpler model are more concentrated around its mean, which indicates that the simpler model is more confident about the estimation.

3. (25 points) Perform posterior predictive checks for both models, being sure to explain what you are doing. Which model do you think fits the data better?

    **Solution**:

    The posterior predictive checks (PPC) are performed to evaluate the goodness-of-fit of the models. The PPC is done by comparing the observed data with the data simulated from the posterior predictive distribution. Figure 3 shows the posterior predictive distributions of the two models. The results show that the simpler model fits data better than the full model since the posterior predictive distribution of the simpler model is more concentrated around the observed data. As a result, the observed data should have a higher posterior predictive density under the simpler model.

    ```{r fig.cap = "Overlay of the Posterior Predictive Distributions.", message = FALSE}
    y_hat <- extract(nfit)$y_hat
    y_hat_simple <- extract(nfit_simple)$y_hat
    plot1 <- ppc_dens_overlay(
      yrep = log(y_hat + 1), y = log(data$y + 1)
    ) + ggplot2::labs(
      title = "Full Model", x = expression("log(y + 1)"), y = "Density"
    )
    plot2 <- ppc_dens_overlay(
      yrep = log(y_hat_simple + 1), y = log(data$y + 1)
    ) + ggplot2::labs(
      title = "Simple Model", x = expression("log(y + 1)"), y = "Density"
    )
    plot1 + plot2
    ```


4. (25 points) Use the results from the second model to create a contour plot showing the log average Poisson intensity as a function of $x$. In other words, plot $\log\mathbb{E}_{p(\beta|\boldsymbol{x},\boldsymbol{y})}[\exp\left(f(\boldsymbol{x},\boldsymbol{\beta})\right)]$ as a function of the two components of $x$ (you can restrict the component ranges from $-10$ to $+10$).

    **Solution**:

    The code below generates the contour plot as requested.

    ```{r echo = FALSE, eval = FALSE}
    log_avg_intensity <- function(x1, x2, weights) {
      intensity <- weights[, 1] * x1 +
        weights[, 2] * x2 +
        weights[, 3] * x2^2 +
        weights[, 4]
      out <- log(mean(exp(intensity)))
      return(out)
    }

    x1 <- seq(-10, 10, 0.5)
    x2 <- seq(-10, 10, 0.5)
    grid <- expand.grid(x1 = x1, x2 = x2)
    weight_samples <- samples_simple[, 1:4]
    z <- mapply(
      function(x1, x2) log_avg_intensity(x1, x2, weight_samples),
      grid$x1,
      grid$x2
    )
    z <- matrix(z, nrow = length(x1), ncol = length(x2))
    col <- hcl.colors(10, "YlOrRd")
    contour(
      x1,
      x2,
      z,
      col = col,
      xlab = expression("x[1]"),
      ylab = expression("x[2]"),
      main = "Log Average Intensity"
    )
    ```

\pagebreak

# Applied Problem

Scientists at the Notlem lab are researching the conversion of stem cells into pancreatic $\beta$-cells to treat patients with type $1$ diabetes. They recently developed two new chmical modulators for improved conversion, and wish to identify their **concentration settings that maximize conversion**.

The scientists also state that:

- their resource consraints limit the *number of plates to $3$*. Since each plate has $24$ wells, you can get **at most $72$ measurements**.
- their time constraints limit the *number of experimental runs to $2$*. Thus you can tell the scientists $24$ pairs of concentrations followed by $48$, or you can tell them $48$ pairs followed by $24$. In either case, you can wait for their measurements for the first set before choosing the concentrations for the second.
- you should only consider concentrations between **$0$ and $80$ for the first chemical modulator**, and between **$0$ and $30$ for the second chemical modulator**, because anything outside this region is not thought to improve conversion. and that
- **polynomial models of conversion** as a function of chemical modulators' concentrations (*e.g., $f(x,\beta)$ like the previous question, but possibly with cubic or even higher terms*) have been shown to enjoy some measure of success for this type of problem in previous literature. (Unlike the earlier question, here the measurements are **real valued**, so you don't need the $\exp$ and Poisson parts, just Gaussian noise.)

Your task in this problem is to:

- tell the scientists the specific pairs of concentrations to run in the study, explaining your thinking
- analyze the resulting (cumulative) data to build a Bayesian regression model of stem cell conversion as a function of the two chemical modulators' concentrations,
- use the model to create a **contour plot** of the posterior predictive mean of conversion as a function of concentration settings,
- use the model to calculate the **posterior predictive distribution** of the concentraition settings that yield maximum conversion, and finally
- construct the posterior predictive distribution of the conversion corresponding to *a specific point prediction of the concentration* that yields **maximum conversion**.

**Solution**:

In this problem, we formulate the Gaussian linear regression model. Denote the conversion rate as $\boldsymbol{y} = \left\{y_{1},\ldots,y_{n}\right\}$, and the concentration of the pair of chemical modulators as $\boldsymbol{x}_{i} = (x_{i,1}, x_{i,2})$, the model is formulated as
$$
  y_{i} | \beta \sim \mathcal{N}(f(\boldsymbol{x}_{i}, \beta), \sigma^{2}).
$$

Since we have already known that the concentration of the first chemical modulator is between $0$ and $80$, and the concentration of the second chemical modulator is between $0$ and $30$, it is reasonable to introduce two new variables $z_{1}$ and $z_{2}$ such that
$$
  x_{i,1} = \frac{80}{1 + e^{-z_{1}}}, \quad x_{i,2} = \frac{30}{1 + e^{-z_{2}}}.
$$

```{r}
# Read data
if (file.exists("data/hw2_init_result.csv")) {
  df_1 <- read.csv("data/hw2_init_result.csv", header = FALSE, sep = " ")
} else {
  stop("FileNotFound: data not found at 'data/hw2_init_result.csv'.")
}
colnames(df_1) <- c("x1", "x2", "y")
summary(df_1)

# Preprocess the features through polynomial expansion
df_1$z1 <- log(df_1$x1)
```

According to the visualization, the conversion rate is
