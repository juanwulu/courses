---
title: |
    | **STAT 656: Bayesian Data Analysis**
    | **Fall 2024**
    | **Homework 1**
author: "Juanwu Lu^[College of Engineering, Purdue University, West Lafayette, IN, USA]"
description: "Homework 1 for STAT 656 at Purdue University"
output: pdf_document
---

# Synthetic Data

The _autoregressive model_ is frequently used to analyze time series data. The simplest autoregressive model has order 1, and is abbreviated as AR(1). This model assumes that an observation $y_i$ at time point $i\ (i=1,\ldots,n)$ is generated according to $$y_i=\rho y_{i-1}+\epsilon_i,$$ where $\epsilon_i\sim\mathcal{N}(0,\sigma^2)$ independently, and $rho$ and $\sigma$ are unknown parameters. For simplicity, we shall assume that $y_0$ is a fixed constant. We will also assume $|\rho|<1$.

1. (5 points) **Solution:**

    Given the formulation above, the log-likelihood function is calculated as follows:
    $$
    \begin{aligned}
        \log L(\rho, \sigma^2|y_0, y_1,\ldots,y_n) &= \log\prod\limits_{i=1}^n (2\pi\sigma^2)^{-\frac{1}{2}}\cdot\exp\left\{-\frac{(y_i-\rho y_{i-1})^2}{2\sigma^2}\right\} \\
        &= -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2 \\
        &= -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2.
    \end{aligned}
    $$

2. (10 points) **Solution:**

    The code implementation is as follows:
    ```{r fig.width=8, fig.height=6}
    # Read data
    if (file.exists("data/computation_data_hw_1.csv")) {
      data <- read.csv("data/computation_data_hw_1.csv")
      y <- data[["x"]]
    } else {
      stop("Cannot find the file 'data/computation_data_hw_1.csv' at ", getwd())
    }

    # Define the log-likelihood function
    ar_loglik <- function(rho, log_sig) {
      y <- .GlobalEnv$y
      n <- length(y)
      sig <- exp(log_sig)
      rho <- rep(as.numeric(rho), times = n - 1)
      denom <- n * log(2 * pi) + 2 * n * log_sig
      log_lik <- -0.5 * (
        denom + (y[1]^2 + sum((y[2:n] - rho * y[1:(n - 1)])^2)) / sig^2
      )
      return(log_lik)
    }

    # Visualization
    rho <- seq(-0.99, 0.99, length = 100)
    log_sig <- seq(-1.0, 0.0, length = 100)
    loglik <- outer(rho, log_sig, Vectorize(ar_loglik))
    contour(
      x = rho,
      y = log_sig,
      z = loglik,
      xlab = expression(rho),
      ylab = expression(log(sigma)),
      nlevels = 20,
      axes = FALSE,
    )
    axis(side = 1, at = seq(-1.0, 1.0, by = 0.25))
    axis(side = 2, at = seq(-1.0, 0.0, by = 0.10))
    ```

3. (10 points) **Solution:**

    Since the prior is independent, we have $p(\rho,\log(\sigma))=p(\rho)p(\log(\sigma))=\frac{1}{2}\cdot\frac{1}{10\sqrt{2\pi}}\exp\left\{-\frac{\log(\sigma)^2}{200}\right\}$. Therefore, the posterior density is proportional to the product of the likelihood and the prior, and then the log of the posterior density is calculated up to a constant as follows:
    $$
    \begin{aligned}
        \log p(\rho,\log(\sigma)|y_0,y_1,\ldots,y_n) &= \log L(\rho,\log(\sigma)|y_0,y_1,\ldots,y_n) + \log p(\rho,\log(\sigma)) + C \\
        &= - n\log(\sigma) - \frac{1}{2\exp(2 * \log(\sigma))}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2 - \frac{\log(\sigma)^2}{200} + C^\prime
    \end{aligned}
    $$
    where $C$ is the constant log-normalizer and $C^\prime$ is a constant irrelevant to $\rho$ and $\log(\sigma)$. The visualization of the log of the posterior density is as follows:
    ```{r fig.width=8, fig.height=6}
    ar_logpost <- function(rho, log_sig) {
      log_prior <- -log(2) - 0.5 * (log(2 * pi) + log(100) + log_sig^2 / 100)
      log_post <- ar_loglik(rho, log_sig) + log_prior
      return(log_post)
    }

    logpost <- outer(rho, log_sig, Vectorize(ar_logpost))
    contour(
      x = rho,
      y = log_sig,
      z = logpost,
      xlab = expression(rho),
      ylab = expression(log(sigma)),
      nlevels = 20,
      axes = FALSE,
    )
    axis(side = 1, at = seq(-1.0, 1.0, by = 0.25))
    axis(side = 2, at = seq(-1.0, 0.0, by = 0.10))
    ```

    Compared to the log-likelihood function, the log posterior density is more concentrated around the maximum likelihood estimate of $(\rho, \log(\sigma))^\intercal$. The prior is not overly informative since the shape of the posterior density is still similar to the likelihood function, indicating that the posterior is mainly determined by the likelihood function.

4. (10 points) **Solution:**

    From the visualization in 3., we can see that the posterior density is concentrated around $0.25\leq\rho\leq 0.75$ and $-0.9\leq\log(\sigma)\leq -0.5$. Therefore, we can choose a grid of $\rho$ and $\log(\sigma)$ as follows:
    ```{r}
    set.seed(42)
    rho_grid <- seq(0.25, 0.75, length = 100)
    log_sig_grid <- seq(-0.9, -0.5, length = 100)
    grid_log_post <- outer(rho_grid, log_sig_grid, Vectorize(ar_logpost))
    # Calculate the normalized probability density
    probs <- exp(grid_log_post - max(grid_log_post))
    probs <- probs / sum(probs)
    # Randomly sample from the grid
    indices <- sample(
      x = seq_len(length(as.vector(probs))),
      size = 1000,
      replace = TRUE,
      prob = probs
    )
    rho_sample <- rho_grid[((indices - 1) %% nrow(probs)) + 1]
    log_sig_sample <- log_sig_grid[((indices - 1) %/% nrow(probs)) + 1]
    ```

5. (5 points) **Solution:**

    The code implementation is as follows:
    ```{r}
    library(moments)

    # Calculate summaries for rho
    print(quantile(rho_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(rho_sample))
    sprintf("Standard Deviation: %.4f", sd(rho_sample))
    sprintf("Skewnewss: %.4f", skewness(rho_sample))
    sprintf("Kurtosis: %.4f", kurtosis(rho_sample))
    # Calculate summaries for log(sigma)
    print(quantile(log_sig_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(log_sig_sample))
    sprintf("Standard Deviation: %.4f", sd(log_sig_sample))
    sprintf("Skewness: %.4f", skewness(log_sig_sample))
    sprintf("Kurtosis: %.4f", kurtosis(log_sig_sample))
    ```


6. (10 points) **Solution:**

    The code implementation is as follows:
    ```{r}
    ar_post_predictive <- function(rho, log_sig) {
      # Sample from the posterior predictive distribution
      y <- .GlobalEnv$y
      n <- length(y)
      new_y <- rep(0.0, times = n)
      sig <- exp(log_sig)
      for (i in 2:n) {
        new_y[i] <- rnorm(n = 1, mean = rho * new_y[i - 1], sd = sig)
      }
      return(new_y)
    }

    params <- data.frame(rho = rho_sample, log_sig = log_sig_sample)
    samples <- matrix(NA, nrow = nrow(params), ncol = length(y))
    for (i in seq_len(nrow(params))) {
      samples[i, ] <- ar_post_predictive(params[i, "rho"], params[i, "log_sig"])
    }
    ```

    The above code makes use of the grid samples drawn in problem 4. For each pair of $(\rho, \log(\sigma))$, we use the AR(1) model to generate a new sample of sequences. The summary statistics of the posterior predictive distribution are as follows:
    ```{r}
    print("Sequence means:")
    print(colMeans(samples))
    print("Sequence standard deviations:")
    print(apply(samples, 2, sd))
    ```

7. (10 points) **Solution:**

    The visualization of the posterior predictive samples against the observed data is as follows:
    ```{r fig.width=8, fig.height=6}
    plot(
      x = seq_len(length(y)),
      y = samples[1, ],
      col = "gray",
      type = "l",
      lwd = 0.75,
      xlab = "Time",
      ylab = "Value",
      ylim = c(-3.0, 3.0)
    )
    for (i in 2:100) {
      lines(x = seq_len(length(y)), y = samples[i, ], col = "gray", lwd = 0.75)
    }
    lines(x = seq_len(length(y)), y = y, col = "blue", lwd = 1.0)
    legend(
      "topright",
      legend = c("Observed", "Posterior Predictive Samples"),
      col = c("blue", "gray"),
      lwd = c(1, 0.5),
      bg = "white",
    )
    ```

    From the visualization, we see that the posterior predictive samples have a wider range than the observed data, indicating that the model has a higher uncertainty in prediction and may not be able to capture the true data distribution well. Therefore, the model is **not a good fit** for the observed data. My expectation for a good model is that the posterior predictive samples are close to the observed data with a similar tendency and range.

\newpage

# Real Data

1. (5 points) **Solution:**

    **No**, I do not believe that the information given is sufficient for analyzing the data. The GitHub page has only provided the dataset file, source of data, and definitions of different sorts of cases. All these pieces of information are related to the observation, i.e., COVID-19 cases, but none of them reflect any information to support the prior distribution of the parameters in the model. Other information such as demographic statistics, geofencing data, activity data, etc., can be useful for data analysis.

2. (5 points) **Solution:**

    Before establishing the model, the raw data is visualized as follows:
    ```{r fig.width=8, fig.height=6}
    # Read data
    if (file.exists("data/covid_us.txt")) {
      data <- read.table("data/covid_us.txt", header = TRUE, sep = ",")
      y <- data[["cases"]]
    } else {
      stop("File not found: 'computation_data_hw_1.csv' at ", getwd())
    }
    plot(
      x = seq_len(length(y)),
      y = y,
      type = "l",
      xlab = "Date",
      ylab = "Cases",
      xaxt = "n"
    )
    lines(
      x = seq_len(length(y)),
      y = exp(0.075 * (seq_len(length(y)) - 1)),
      col = "blue"
    )
    axis(side = 1, labels = data[["date"]], at = seq_len(length(y)))
    legend(
      "topleft",
      legend = c("Observation", expression(y=exp(0.075 * (t - 1)))),
      col = c("black", "blue"),
      lwd = c(1, 0.5),
      bg = "white",
    )
    ```
    From the visualization, it is clear that the raw data has an exponential growth trend (blue line), which can not be directly captured by the AR(1) model. Therefore, the raw data needs to be transformed before being used for fitting an AR(1) model. The transformation consists of two steps: first, taking the logarithm of the raw data, and then, differentiating the transformed with `lag = 1` to filter out the non-stationary trend that is not captured by the AR(1) model. The transformed data is given as
    $$
    \omega_t = \log(y_t) - \log(y_{t-1}).
    $$
    For consistency, $\omega_1 = 0$. The visualization of the transformed data is as follows:
    ```{r fig.width=8, fig.height=6}
    omg <- c(0, diff(log(y), lag = 1))
    plot(
      x = omg[seq(1, length(omg) - 1)],
      y = omg[seq(2, length(omg))],
      type = "p",
      pch = 21,
      col = "blue",
      bg = "lightblue",
      xlab = expression(omega[t - 1]),
      ylab = expression(omega[t]),
    )
    ```
    As shown in the visualization, the transformed data has a more linear relationship between consecutive observations. Finally, the AR(1) model is constructed as:
    $$
      \omega_i = \rho\omega_{i-1} + \epsilon_i, \quad \epsilon_i\sim\mathcal{N}(0, \sigma^2).
    $$

3. (5 points) **Solution:**

    Based on the visualization above, a majority of the point cloud is growing linearly with a slope around $1.0$ and slopes are non-negative. Therefore, the prior distribution of $\rho$ can be set as a Gamma distribution $\rho\sim\text{Gamma}(1, 1)$ with a mean of $1.0$. For the log-variance $\log(\sigma)$, without losing generality, the prior can be set a normal distribution $\log(\sigma)\sim\mathcal{N}(0, 1^2)$.

4. (10 points) **Solution:**

5.
