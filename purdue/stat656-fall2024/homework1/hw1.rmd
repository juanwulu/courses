---
title: |
    | **STAT 656: Bayesian Data Analysis**
    | **Fall 2024**
    | **Homework 1**
author: "Juanwu Lu^[College of Engineering, Purdue University, West Lafayette, IN, USA]"
description: "Homework 1 for STAT 656 at Purdue University"
output: pdf_document
---

## Synthetic Data

The _autoregressive model_ is frequently used to analyze time series data. The simplest autoregressive model has order 1, and is abbreviated as AR(1). This model assumes that an observation $y_i$ at time point $i\ (i=1,\ldots,n)$ is generated according to $$y_i=\rho y_{i-1}+\epsilon_i,$$ where $\epsilon_i\sim\mathcal{N}(0,\sigma^2)$ independently, and $rho$ and $\sigma$ are unknown parameters. For simplicity, we shall assume that $y_0$ is a fixed constant. We will also assume $|\rho|<1$.

1. (5 points) **Solution:**

    Given the formulation above, the log-likelihood function is calculated as follows:
    $$
    \begin{aligned}
        \log L(\rho, \sigma^2|y_0, y_1,\ldots,y_n) &= \log\prod\limits_{i=1}^n (2\pi\sigma^2)^{-\frac{1}{2}}\cdot\exp\left\{-\frac{(y_i-\rho y_{i-1})^2}{2\sigma^2}\right\} \\
        &= -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2 \\
        &= -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2.
    \end{aligned}
    $$

2. (10 points) **Solution:**

    The code implementation is as follows:
    ```{r fig.width=8, fig.height=6}
    # Read data
    if (file.exists("computation_data_hw_1.csv")) {
        data <- read.csv("computation_data_hw_1.csv")
        y <- data[['x']]
    } else {
        stop("Cannot find the data file 'computation_data_hw_1.csv' at ", getwd())
    }

    # Define the log-likelihood function
    ar_loglik <- function(rho, log_sig) {
        n <- length(y)
        sig <- exp(log_sig)
        rho <- rep(as.numeric(rho), times = n - 1)
        log_lik <- -0.5 * (
            n * log(2 * pi)
            + 2 * n * log_sig
            + (y[1]^2 + sum((y[2:n] - rho * y[1:(n-1)])^2)) / sig^2
        )
        return(log_lik)
    }

    # Visualization
    rho <- seq(-0.99, 0.99, length=100)
    log_sig <- seq(-1.0, 0.0, length=100)
    loglik <- outer(rho, log_sig, Vectorize(ar_loglik))
    contour(
        x=rho,
        y=log_sig,
        z=loglik,
        xlab=expression(rho),
        ylab=expression(log(sigma)),
        nlevels=20,
    )
    ```

3. (10 points) **Solution:**

    Since the prior is independent, we have $p(\rho,\log(\sigma))=p(\rho)p(\log(\sigma))=\frac{1}{2}\cdot\frac{1}{10\sqrt{2\pi}}\exp\left\{-\frac{\log(\sigma)^2}{200}\right\}$. Therefore, the posterior density is proportional to the product of the likelihood and the prior, and then the log of the posterior density is calculated up to a constant as follows:
    $$
    \begin{aligned}
        \log p(\rho,\log(\sigma)|y_0,y_1,\ldots,y_n) &= \log L(\rho,\log(\sigma)|y_0,y_1,\ldots,y_n) + \log p(\rho,\log(\sigma)) + C \\
        &= - n\log(\sigma) - \frac{1}{2\exp(2 * \log(\sigma))}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2 - \frac{\log(\sigma)^2}{200} + C^\prime
    \end{aligned}
    $$
    where $C$ is the constant log-normalizer and $C^\prime$ is a constant irrelevant to $\rho$ and $\log(\sigma)$. The visualization of the log of the posterior density is as follows:
    ```{r fig.width=8, fig.height=6}
    # Define the log-likelihood function
    ar_logpost <- function(rho, log_sig) {
        log_prior <- -log(2) - 0.5 * (log(2 * pi) + log(100) + log_sig^2 / 100)
        log_post <- ar_loglik(rho, log_sig) + log_prior
        return(log_post)
    }

    # Visualization
    logpost <- outer(rho, log_sig, Vectorize(ar_logpost))
    contour(
        x=rho,
        y=log_sig,
        z=logpost,
        xlab=expression(rho),
        ylab=expression(log(sigma)),
        nlevels=20,
    )
    ```

    Compared to the log-likelihood function, the log posterior density is more concentrated around the maximum likelihood estimate of $(\rho, \log(\sigma))^\intercal$. The prior is not overly informative since the shape of the posterior density is still similar to the likelihood function, indicating that the posterior is mainly determined by the likelihood function.

4. (10 points) **Solution:**

    From the visualization in 3., we can see that the posterior density is concentrated around $0.0\leq\rho\leq 1.0$ and $-0.9\leq\log(\sigma)\leq 0.0$. Therefore, we can choose a grid of $\rho$ and $\log(\sigma)$ as follows:
    ```{r}
    set.seed(42)
    rho_grid <- sample(x=seq(0.0, 1.0, length=5000), size=1000)
    log_sig_grid <- sample(x=seq(-0.9, 0.0, length=5000), size=1000)
    ```

5. (5 points) **Solution:**

    The code implementation is as follows:
    ```{r}
    library(moments)

    # Calculate summaries for rho
    print(quantile(rho_grid, probs=c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(rho_grid))
    sprintf("Standard Deviation: %.4f", sd(rho_grid))
    sprintf("Skewnewss: %.4f", skewness(rho_grid))
    sprintf("Kurtosis: %.4f", kurtosis(rho_grid))
    # Calculate summaries for log(sigma)
    print(quantile(log_sig_grid, probs=c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(log_sig_grid))
    sprintf("Standard Deviation: %.4f", sd(log_sig_grid))
    sprintf("Skewness: %.4f", skewness(log_sig_grid))
    sprintf("Kurtosis: %.4f", kurtosis(log_sig_grid))
    ```


6. (10 points) **Solution:**

   The code implementation is as follows:
    ```{r}
    ar_post_predictive <- function(rho, log_sig) {
        # Sample from the posterior predictive distribution
        n <- length(y)
        new_y <- rep(0.0, times = n)
        sig <- exp(log_sig)
        for (i in 2:n) {
            new_y[i] <- rnorm(n=1, mean=rho * new_y[i-1], sd=sig)
        }
        return(new_y)
    }

    params <- data.frame(rho=rho_grid, log_sig=log_sig_grid)
    samples <- matrix(NA, nrow=nrow(params), ncol=length(y))
    for (i in 1:nrow(params)) {
        samples[i,] <- ar_post_predictive(params[i, 'rho'], params[i, 'log_sig'])
    }
    ```

    The above code makes use of the grid samples drawn in problem 4. For each pair of $(\rho, \log(\sigma))$, we use the AR(1) model to generate a new sample of sequences. The summary statistics of the posterior predictive distribution are as follows:
    ```{r}
    print("Sequence means:")
    print(colMeans(samples))
    print("Sequence standard deviations:")
    print(apply(samples, 2, sd))
    ```

7. (10 points) **Solution:**

    The visualization of the posterior predictive samples against the observed data is as follows:
    ```{r fig.width=8, fig.height=6}
    plot(
        x=1:length(y),
        y=samples[1,],
        col='gray',
        type='l',
        lwd=0.75,
        xlab='Time',
        ylab='Value',
        ylim=c(-3.0, 3.0)
    )
    for (i in 2:100) {
        lines(x=1:length(y), y=samples[i,], col='gray', lwd=0.75)
    }
    lines(x=1:length(y), y=y, col='blue', lwd=1.0)
    legend(
        'topright',
        legend=c('Observed', 'Posterior Predictive Samples'),
        col=c('blue', 'gray'),
        lwd=c(1, 0.5),
        bg='white',
    )
    ```

    From the visualization, we see that the posterior predictive samples have a wider range than the observed data, indicating that the model has a higher uncertainty in prediction and may not be able to capture the true data distribution well. Therefore, the model is not a good fit for the observed data.
