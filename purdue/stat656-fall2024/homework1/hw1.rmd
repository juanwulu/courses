---
title: |
    | **STAT 656: Bayesian Data Analysis**
    | **Fall 2024**
    | **Homework 1**
author: "Juanwu Lu^[College of Engineering, Purdue University, West Lafayette, IN, USA]"
description: "Homework 1 for STAT 656 at Purdue University"
output: pdf_document
---

# Synthetic Data

The _autoregressive model_ is frequently used to analyze time series data. The simplest autoregressive model has order 1, and is abbreviated as AR(1). This model assumes that an observation $y_i$ at time point $i\ (i=1,\ldots,n)$ is generated according to $$y_i=\rho y_{i-1}+\epsilon_i,$$ where $\epsilon_i\sim\mathcal{N}(0,\sigma^2)$ independently, and $rho$ and $\sigma$ are unknown parameters. For simplicity, we shall assume that $y_0$ is a fixed constant. We will also assume $|\rho|<1$.

1. (5 points) **Solution:**

    Given the formulation above, the log-likelihood function is calculated as follows:
    $$
    \begin{aligned}
        \log L(\rho, \sigma^2|y_0, y_1,\ldots,y_n) &= \log\prod\limits_{i=1}^n (2\pi\sigma^2)^{-\frac{1}{2}}\cdot\exp\left\{-\frac{(y_i-\rho y_{i-1})^2}{2\sigma^2}\right\} \\
        &= -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2 \\
        &= -\frac{n}{2}\log(2\pi) - n\log(\sigma) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2.
    \end{aligned}
    $$

2. (10 points) **Solution:**

    The code implementation is as follows:
    ```{r fig.width=8, fig.height=6}
    # Read data
    if (file.exists("data/computation_data_hw_1.csv")) {
      data <- read.csv("data/computation_data_hw_1.csv")
      y <- data[["x"]]
    } else {
      stop("Cannot find the file 'data/computation_data_hw_1.csv' at ", getwd())
    }

    # Define the log-likelihood function
    ar_loglik <- function(rho, log_sig) {
      y <- .GlobalEnv$y
      n <- length(y)
      sig <- exp(log_sig)
      rho <- rep(as.numeric(rho), times = n - 1)
      denom <- n * log(2 * pi) + 2 * n * log_sig
      log_lik <- -0.5 * (
        denom + (y[1]^2 + sum((y[2:n] - rho * y[1:(n - 1)])^2)) / sig^2
      )
      return(log_lik)
    }

    # Visualization
    rho <- seq(-0.99, 0.99, length = 100)
    log_sig <- seq(-1.0, 0.0, length = 100)
    loglik <- outer(rho, log_sig, Vectorize(ar_loglik))
    contour(
      x = rho,
      y = log_sig,
      z = loglik,
      xlab = expression(rho),
      ylab = expression(log(sigma)),
      main = "Log-Likelihood Function",
      nlevels = 20,
      axes = FALSE,
    )
    axis(side = 1, at = seq(-1.0, 1.0, by = 0.25))
    axis(side = 2, at = seq(-1.0, 0.0, by = 0.10))
    ```

3. (10 points) **Solution:**

    Since the prior is independent, we have $p(\rho,\log(\sigma))=p(\rho)p(\log(\sigma))=\frac{1}{2}\cdot\frac{1}{10\sqrt{2\pi}}\exp\left\{-\frac{\log(\sigma)^2}{200}\right\}$. Therefore, the posterior density is proportional to the product of the likelihood and the prior, and then the log of the posterior density is calculated up to a constant as follows:
    $$
    \begin{aligned}
        \log p(\rho,\log(\sigma)|y_0,y_1,\ldots,y_n) &= \log L(\rho,\log(\sigma)|y_0,y_1,\ldots,y_n) + \log p(\rho,\log(\sigma)) + C \\
        &= - n\log(\sigma) - \frac{1}{2\exp(2 * \log(\sigma))}\sum\limits_{i=1}^n(y_i-\rho y_{i-1})^2 - \frac{\log(\sigma)^2}{200} + C^\prime
    \end{aligned}
    $$
    where $C$ is the constant log-normalizer and $C^\prime$ is a constant irrelevant to $\rho$ and $\log(\sigma)$. The visualization of the log of the posterior density is as follows:
    ```{r fig.width=8, fig.height=6}
    ar_logpost <- function(rho, log_sig) {
      log_prior <- -log(2) - 0.5 * (log(2 * pi) + log(100) + log_sig^2 / 100)
      log_post <- ar_loglik(rho, log_sig) + log_prior
      return(log_post)
    }

    logpost <- outer(rho, log_sig, Vectorize(ar_logpost))
    contour(
      x = rho,
      y = log_sig,
      z = logpost,
      main = "Unnormalized Log-Posterior Density",
      xlab = expression(rho),
      ylab = expression(log(sigma)),
      nlevels = 20,
      axes = FALSE,
    )
    axis(side = 1, at = seq(-1.0, 1.0, by = 0.25))
    axis(side = 2, at = seq(-1.0, 0.0, by = 0.10))
    ```

    Compared to the log-likelihood function, the log posterior density is more concentrated around the maximum likelihood estimate of $(\rho, \log(\sigma))^\intercal$. The prior is not overly informative since the shape of the posterior density is still similar to the likelihood function, indicating that the posterior is **mainly determined by the likelihood function**.

4. (10 points) **Solution:**

    From the visualization in 3., we can see that the posterior density is concentrated around $0.25\leq\rho\leq 0.75$ and $-0.9\leq\log(\sigma)\leq -0.5$. Therefore, we can choose a grid of $\rho$ and $\log(\sigma)$ as follows:
    ```{r}
    set.seed(42)
    rho_grid <- seq(0.25, 0.75, length = 100)
    log_sig_grid <- seq(-0.90, -0.50, length = 100)
    grid_log_post <- outer(rho_grid, log_sig_grid, Vectorize(ar_logpost))
    # Calculate the normalized probability density
    probs <- exp(grid_log_post - max(grid_log_post))
    probs <- probs / sum(probs)
    # Randomly sample from the grid
    indices <- sample(
      x = seq_len(length(as.vector(probs))),
      size = 1000,
      replace = TRUE,
      prob = probs
    )
    rho_sample <- rho_grid[((indices - 1) %% nrow(probs)) + 1]
    log_sig_sample <- log_sig_grid[((indices - 1) %/% nrow(probs)) + 1]
    ```

5. (5 points) **Solution:**

    The code implementation is as follows:
    ```{r}
    library(moments)

    # Calculate summaries for rho
    print(quantile(rho_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(rho_sample))
    sprintf("Standard Deviation: %.4f", sd(rho_sample))
    sprintf("Skewnewss: %.4f", skewness(rho_sample))
    sprintf("Kurtosis: %.4f", kurtosis(rho_sample))
    # Calculate summaries for log(sigma)
    print(quantile(log_sig_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(log_sig_sample))
    sprintf("Standard Deviation: %.4f", sd(log_sig_sample))
    sprintf("Skewness: %.4f", skewness(log_sig_sample))
    sprintf("Kurtosis: %.4f", kurtosis(log_sig_sample))
    ```


6. (10 points) **Solution:**

    The code implementation is as follows:
    ```{r}
    ar_post_predictive <- function(rho, log_sig) {
      # Sample from the posterior predictive distribution
      y <- .GlobalEnv$y
      new_y <- rep(0.0, times = length(y))
      sig <- exp(log_sig)
      for (i in 2:length(y)) {
        new_y[i] <- rnorm(n = 1, mean = rho * y[i - 1], sd = sig)
      }
      return(new_y)
    }

    params <- data.frame(rho = rho_sample, log_sig = log_sig_sample)
    samples <- matrix(NA, nrow = nrow(params), ncol = length(y))
    for (i in seq_len(nrow(params))) {
      samples[i, ] <- ar_post_predictive(params[i, "rho"], params[i, "log_sig"])
    }
    ```

    The above code makes use of the grid samples drawn in problem 4. For each pair of $(\rho, \log(\sigma))$, we use the AR(1) model to generate a new sample of sequences. The summary statistics of the posterior predictive distribution are as follows:
    ```{r}
    print("Sequence means:")
    print(colMeans(samples))
    print("Sequence standard deviations:")
    print(apply(samples, 2, sd))
    ```

7. (10 points) **Solution:**

    The visualization of the posterior predictive samples against the observed data is as follows:
    ```{r fig.width=8, fig.height=6}
    plot(
      x = seq_len(length(y)),
      y = samples[1, ],
      col = "gray",
      type = "l",
      lwd = 0.75,
      main = "Posterior Predictive Samples vs. Observed Data",
      xlab = "Time",
      ylab = "Value",
      ylim = c(-3.0, 3.0)
    )
    for (i in 2:100) {
      lines(x = seq_len(length(y)), y = samples[i, ], col = "gray", lwd = 0.75)
    }
    lines(x = seq_len(length(y)), y = y, col = "blue", lwd = 1.0)
    legend(
      "topright",
      legend = c("Observed", "Posterior Predictive Samples"),
      col = c("blue", "gray"),
      lwd = c(1, 0.5),
      bg = "white",
    )
    ```

    From the visualization, we see that the posterior predictive samples have a wider range than the observed data, indicating that the model has a higher uncertainty in prediction and may not be able to capture the true data distribution well. Therefore, the model is **not a good fit** for the observed data. My expectation for a good model is that the posterior predictive samples are close to the observed data with a similar tendency and range.

\newpage

# Real Data

1. (5 points) **Solution:**

    **No**, I do not believe that the information given is sufficient for analyzing the data. The GitHub page has only provided the dataset file, source of data, and definitions of different sorts of cases. All these pieces of information are related to the observation, i.e., COVID-19 cases, but none of them reflect any information to support the prior distribution of the parameters in the model. Other information such as demographic statistics, geofencing data, activity data, etc., can be useful for data analysis.

2. (5 points) **Solution:**

    Before establishing the model, the raw data is visualized as follows:
    ```{r fig.width=8, fig.height=6}
    # Read data
    if (file.exists("data/covid_us.txt")) {
      data <- read.table("data/covid_us.txt", header = TRUE, sep = ",")
      y <- data[["cases"]]
    } else {
      stop("File not found: 'computation_data_hw_1.csv' at ", getwd())
    }
    plot(
      x = seq_len(length(y)),
      y = y,
      type = "l",
      main = "COVID-19 Cases in the US",
      xlab = "Date",
      ylab = "Cases",
      xaxt = "n"
    )
    axis(side = 1, labels = data[["date"]], at = seq_len(length(y)))
    legend(
      "topleft",
      legend = c("Observation", expression(y=exp(0.075 * (t - 1)))),
      col = c("black", "blue"),
      lwd = c(1, 0.5),
      bg = "white",
    )
    ```

    From the visualization, it is clear that the raw data has an exponential growth trend (blue line), which can not be directly captured by the AR(1) model. Therefore, the raw data needs to be transformed before being used for fitting an AR(1) model. The transformation consists of two steps: first, differentiating the transformed with `lag = 1` to filter out the non-stationary trend that is not captured by the AR(1) model, and then, scale the differentiated data by a factor of $1/100000$ to make the data lying in a reasonable range:
    $$
    \omega_t = \frac{1}{100000}(y_t - y_{t-1}).
    $$
    For consistency, $\omega_1 = 0$. The visualization of the transformed data is as follows:
    ```{r fig.width=8, fig.height=6}
    omg <- c(0, diff(y, lag = 1) / 100000)
    plot(
      x = omg[seq(1, length(omg) - 1)],
      y = omg[seq(2, length(omg))],
      type = "p",
      pch = 21,
      col = "blue",
      bg = "lightblue",
      main = "Visualization of Transformed Data",
      xlab = expression(omega[t - 1]),
      ylab = expression(omega[t]),
    )
    ```

    As shown in the visualization, the transformed data has a more linear relationship between consecutive observations. Finally, the AR(1) model is constructed as:
    $$
      \omega_i = \rho\omega_{i-1} + \epsilon_i, \quad \epsilon_i\sim\mathcal{N}(0, \sigma^2).
    $$

3. (5 points) **Solution:**

    Based on the visualization above, a majority of the point cloud is growing linearly with a slope around $1.0$ and slopes are non-negative. Therefore, the prior distribution of $\rho$ can be set as a Gamma distribution $\rho\sim\text{Gamma}(1, 1)$ with a mean of $1.0$. For the log-variance $\log(\sigma)$, without losing generality, the prior can be set a normal distribution $\log(\sigma)\sim\mathcal{N}(0, 1^2)$.

4. (10 points) **Solution:**

    With the above specified prior distributions, the log-posterior density is calculated as follows:
    $$
    \begin{aligned}
        \log p(\rho, \log(\sigma)|\omega_1,\ldots,\omega_n) &= \log L(\rho, \log(\sigma)|\omega_1,\ldots,\omega_n) + \log p(\rho) + \log p(\log(\sigma)) + C \\
        &= -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(\omega_i-\rho\omega_{i-1})^2 + \log\left(\frac{1}{\Gamma(1)}\exp(-\rho)\right) - \frac{\log(\sigma)^2}{2} + C^\prime \\
        &= -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum\limits_{i=1}^n(\omega_i-\rho\omega_{i-1})^2 - \rho - \frac{\log(\sigma)^2}{2} + C^{\prime},
    \end{aligned}
    $$

    Therefore, the model fitted using data from the first time point until June 30 is as follows:
    ```{r fig.width=8, fig.height=6}
    y <- c(
      0,
      diff(y[as.Date(data$date) < as.Date("2020-06-30")], lag = 1) / 100000
    )
    rho <- seq(0.01, 2.0, length = 100)
    log_sig <- seq(-5.0, 0.0, length = 100)
    log_lik <- outer(rho, log_sig, Vectorize(ar_loglik))
    contour(
      x = rho,
      y = log_sig,
      z = log_lik,
      main = "Log-Likelihood Function",
      xlab = expression(rho),
      ylab = expression(log(sigma)),
      nlevels = 20,
    )
    ```

    The visualization of the logarithm of unnormalized posterior density is shown below:
    ```{r fig.width=8, fig.height=6}
    ar_logpost <- function(rho, log_sig) {
      log_prior <- -rho - 0.5 * (log_sig * log_sig)
      log_post <- ar_loglik(rho, log_sig) + log_prior
      return(log_post)
    }
    log_post <- outer(rho, log_sig, Vectorize(ar_logpost))
    contour(
      x = rho,
      y = log_sig,
      z = log_post,
      main = "Unnormalized Log-Posterior Density",
      xlab = expression(rho),
      ylab = expression(log(sigma)),
      nlevels = 20,
    )
    ```

   Similarly, I draw 1000 samples from the grid and calculate the summary statistics as follows:
    ```{r}
    library(moments)

    grid_log_post <- outer(rho, log_sig, Vectorize(ar_logpost))
    # Calculate the normalized probability density
    probs <- exp(grid_log_post - max(grid_log_post))
    probs <- probs / sum(probs)
    print(dim(probs))
    # Randomly sample from the grid
    indices <- sample(
      x = seq_len(length(as.vector(probs))),
      size = 1000,
      replace = TRUE,
      prob = probs
    )
    rho_sample <- rho[((indices - 1) %% nrow(probs)) + 1]
    log_sig_sample <- log_sig[((indices - 1) %/% nrow(probs)) + 1]

    # Calculate summaries for rho
    print(quantile(rho_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(rho_sample))
    sprintf("Standard Deviation: %.4f", sd(rho_sample))
    sprintf("Skewnewss: %.4f", skewness(rho_sample))
    sprintf("Kurtosis: %.4f", kurtosis(rho_sample))
    # Calculate summaries for log(sigma)
    print(quantile(log_sig_sample, probs = c(0.025, 0.25, 0.5, 0.75, 0.975)))
    sprintf("Mean: %.4f", mean(log_sig_sample))
    sprintf("Standard Deviation: %.4f", sd(log_sig_sample))
    sprintf("Skewness: %.4f", skewness(log_sig_sample))
    sprintf("Kurtosis: %.4f", kurtosis(log_sig_sample))
    ```

5. (10 points) **Solution:**

    Similarly, the posterior predictive samples are compared to the observed transformed data. The code implementation is as follows:
    ```{r, fig.width=8, fig.height=6}
    params <- data.frame(rho = rho_sample, log_sig = log_sig_sample)
    samples <- matrix(NA, nrow = nrow(params), ncol = length(y))
    for (i in seq_len(nrow(params))) {
      samples[i, ] <- ar_post_predictive(params[i, "rho"], params[i, "log_sig"])
    }

    # Visualize scatter plot
    plot(
      x = seq_along(samples[1, seq(1, length(y))]),
      y = samples[1, seq(1, length(y))],
      type = "l",
      col = "gray",
      xlab = "Time",
      ylab = expression(omega[t]),
      main = "Posterior Predictive Samples vs. Observed Data",
    )
    for (i in 2:1000) {
      lines(x = seq_along(samples[i]), y = samples[i], col = "gray")
    }
    lines(x = seq_along(y), y = y, col = "blue")
    ```

    Following the posterior predictive check covered in the lecture, the visualization of the skewness and kurtosis of the posterior predictive samples against the observed data is as follows:
    ```{r fig.width=8, fig.height=6}
    skewness_obs <- skewness(y[seq(2, length(y))])
    sample_skewness <- apply(samples, 1, function(x) skewness(x[seq(2, length(y))]))
    kurtosis_obs <- kurtosis(y[seq(2, length(y))])
    sample_kurtosis <- apply(samples, 1, function(x) kurtosis(x[seq(2, length(y))]))

    hist(
      x = sample_skewness,
      main = "Skewness of Posterior Predictive Samples",
      xlab = "Skewness",
      col = "gray",
      border = "white",
    )
    lines(
      x = c(skewness_obs, skewness_obs),
      y = c(0, 300),
      col = "black",
      lwd = 2,
    )
    text(
      x = skewness_obs + 0.05,
      y = 200,
      labels = paste("Observed Skewness = ", round(skewness_obs, 2)),
      pos = 1
    )
    hist(
      x = sample_kurtosis,
      main = "Kurtosis of Posterior Predictive Samples",
      xlab = "Kurtosis",
      col = "gray",
      border = "white",
    )
    lines(
      x = c(kurtosis_obs, kurtosis_obs),
      y = c(0, 500),
      col = "black",
      lwd = 2,
    )
    text(
      x = kurtosis_obs + 0.15,
      y = 300,
      labels = paste("Observed Kurtosis = ", round(kurtosis_obs, 2)),
      pos = 1
    )
    ```

    Based on the posterior predictive checks, the model provides good predictions for the data until June 30. The skewness and kurtosis of the observed data are close to the expected values from the posterior predictive samples, and the time-series visualization shows that the posterior predictive samples are close to the observed data.

6. (10 points) Simulate posterior predictions for the cases and deaths in the United States from July 1 until August 25. Compare these posterior predictions to the actual values of the two variables, and comment on the quality of the AR(1) model for predicting future cases and deaths in this context.

7. (10 points) Now replicate the analyses you performed in step 4 on the entire United States for the state of Indiana individually. Discuss if you think the results agree with the results of the entire United States.

8. (5 points) Provide a non-technical explanation of your finds for an audience with minimal statistical training. Specifically, describe the AR(1) model in lay terms, explain whether or not this model provides good predictions in this context, and provide intuitive justifications for why the AR(1) model succeeds or fails in this context.
